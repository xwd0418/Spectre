{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03/01/23: Trying to load a ranked transformer for testing in plotly/dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "from textwrap import wrap\n",
    "from matplotlib import pyplot as plt\n",
    "from models.ranked_transformer import HsqcRankedTransformer\n",
    "from models.ranked_double_transformer import DoubleTransformer\n",
    "from datasets.generic_index_dataset import GenericIndexedModule\n",
    "from datasets.dataset_utils import pad\n",
    "from utils.ranker import RankingSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/smart4.5/new_split/lr_1e-5/checkpoints/epoch=0-step=3432.ckpt\n"
     ]
    }
   ],
   "source": [
    "# model path\n",
    "folder = Path(\"/data/smart4.5/new_split\")\n",
    "experiment = Path(\"lr_1e-5\")\n",
    "chkpts = list(f for f in os.listdir(folder / experiment / \"checkpoints\") if re.search(\"epoch\", f))\n",
    "chkpt = chkpts[-1] if len(chkpts) else None\n",
    "full_path = os.path.join(folder / experiment / \"checkpoints\" / chkpt) if chkpt is not None else None\n",
    "print(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HsqcRankedTransformer(\n",
       "  (enc): CoordinateEncoder()\n",
       "  (loss): BCEWithLogitsLoss()\n",
       "  (fc): Linear(in_features=128, out_features=6144, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (6): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (7): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model and data\n",
    "model = HsqcRankedTransformer.load_from_checkpoint(full_path).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMILES_dataset_path = \"tempdata/SMILES_dataset\"\n",
    "features = [\"HSQC\", \"R2-6144FP\", \"Chemical\", \"SMILES\"]\n",
    "feature_handlers = [pad, None, None, None]\n",
    "dm = GenericIndexedModule(SMILES_dataset_path, features, feature_handlers, \n",
    "  batch_size = 64, len_override = 200)\n",
    "dm.setup(\"fit\")\n",
    "\n",
    "val_dl = dm.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/smart4.5/datasets/dataset_utils.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence = pad_sequence([torch.tensor(v, dtype=torch.float) for v in sequence], batch_first=True)\n",
      "/workspace/smart4.5/datasets/dataset_utils.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence = pad_sequence([torch.tensor(v, dtype=torch.float) for v in sequence], batch_first=True)\n",
      "/workspace/smart4.5/datasets/dataset_utils.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence = pad_sequence([torch.tensor(v, dtype=torch.float) for v in sequence], batch_first=True)\n",
      "/workspace/smart4.5/datasets/dataset_utils.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence = pad_sequence([torch.tensor(v, dtype=torch.float) for v in sequence], batch_first=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Br.CN1CCc2cc(O)cc3c2C1Cc1ccc(O)c(O)c1-3\n",
      "\"MLS000860019-01!R()-2,10,11-Trihydroxyaporphine hydrobromide\"\n",
      "tensor([[ 1.1852e+02,  6.6382e+00,  5.2922e+03],\n",
      "        [ 1.1515e+02,  6.6885e+00,  4.6099e+03],\n",
      "        [ 1.1366e+02,  6.4267e+00,  3.4196e+03],\n",
      "        [ 1.1263e+02,  7.4786e+00,  8.7722e+03],\n",
      "        [ 6.2320e+01,  3.1595e+00,  5.2923e+03],\n",
      "        [ 5.2830e+01,  3.0983e+00, -8.7723e+03],\n",
      "        [ 5.2830e+01,  2.5967e+00, -8.7723e+03],\n",
      "        [ 4.3570e+01,  2.8437e+00,  2.1039e+04],\n",
      "        [ 3.4300e+01,  3.1175e+00, -5.4442e+03],\n",
      "        [ 3.4300e+01,  2.3950e+00, -5.4442e+03],\n",
      "        [ 2.9990e+01,  3.2700e+00, -6.1525e+03],\n",
      "        [ 2.9990e+01,  2.7900e+00, -5.4498e+03],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "for batch in val_dl:\n",
    "  hsqc, fp, chemical, smiles = batch\n",
    "  print(smiles[0])\n",
    "  print(chemical[0])\n",
    "  print(hsqc[0])\n",
    "  # hsqc, fp = hsqc.to(model.device), fp.to(model.device)\n",
    "  # out = model(hsqc)\n",
    "  # out_fp = torch.where(out > 0, 1, 0)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting samples from the dataset\n",
    "def extract_sample(idx):\n",
    "    hsqc, ms, label = dm.train[idx]\n",
    "    hsqc, ms, label = hsqc.cuda(), ms.cuda(), label.cuda()\n",
    "    out = model.forward(hsqc.unsqueeze(0), ms.unsqueeze(0))[0]\n",
    "    ds_out, ds_label = torch.sigmoid(out), label\n",
    "    ds_out = (ds_out >= 0.5).float()\n",
    "    return ds_out, ds_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole, SimilarityMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def molDiff(smiles1, smiles2):\n",
    "    mol_1 = Chem.MolFromSmiles(smiles1)\n",
    "    mol_2 = Chem.MolFromSmiles(smiles2)\n",
    "    return SimilarityMaps.GetSimilarityMapForFingerprint(mol_1, mol_2, SimilarityMaps.GetMorganFingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    im = Chem.Draw.MolToImage(mol, size=(700,600))\n",
    "    fig, axs = plt.subplots(1, 1, facecolor=\"white\")\n",
    "    axs.imshow(im)\n",
    "    axs.set_title(\"\\n\".join(wrap(smiles, width=35)))\n",
    "    display(fig)\n",
    "def smiles_to_image(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    im = Chem.Draw.MolToImage(mol, size=(700,600))\n",
    "    return im\n",
    "from notebooks.dataset_building import fingerprint_utils\n",
    "def smiles_to_fp(smiles):\n",
    "    return fingerprint_utils.FP_generator(smiles, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_retrieval_image(sample_idx, out_folder):\n",
    "    ds_out, ds_label = extract_sample(sample_idx)\n",
    "    nz_out, nz_label = ranker.normalized_to_nonzero(ds_out), ranker.normalized_to_nonzero(ds_label) #\n",
    "    similarity = F.cosine_similarity(ds_out.unsqueeze(0), ds_label.unsqueeze(0))\n",
    "    original_smiles = ranker.lookup.get(nz_label, None)\n",
    "    single = list(original_smiles)[0] # \n",
    "    im = smiles_to_image(single) #\n",
    "    \n",
    "    results = ranker.retrieve(ds_out)\n",
    "    rank = ranker.batched_rank(ds_out.unsqueeze(0), ds_label.unsqueeze(0))[0].item() #\n",
    "    out_imgs = []\n",
    "    out_smiles = []\n",
    "    out_fps = []\n",
    "    for i, v in enumerate(results):\n",
    "        smiles= list(v)\n",
    "        images = [smiles_to_image(f) for f in smiles]\n",
    "        fps = [ranker.normalized_to_nonzero(torch.tensor(smiles_to_fp(f))) for f in smiles]\n",
    "        out_imgs.append(images)\n",
    "        out_smiles.append(smiles)\n",
    "        out_fps.append(fps)\n",
    "    \n",
    "    if not os.path.exists(out_folder):\n",
    "        os.makedirs(out_folder, exist_ok=True)\n",
    "    out_obj = {\n",
    "        \"out_fp\": nz_out,\n",
    "        \"label_fp\": nz_label,\n",
    "        \"cossim\": similarity.item(),\n",
    "        \"original_smiles\": list(original_smiles),\n",
    "        \"single_smiles\": single,\n",
    "        \"rank\": rank,\n",
    "        \"ranked_cts\": [len(v) for v in out_imgs],\n",
    "        \"ranked_smiles\": out_smiles,\n",
    "        \"ranked_fps\": out_fps\n",
    "    }\n",
    "    im.save(os.path.join(out_folder, \"img.png\"))\n",
    "    for i, v in enumerate(out_imgs):\n",
    "        for j, q in enumerate(v):\n",
    "            q.save(os.path.join(out_folder, f\"img_{i}_{j}.png\"))\n",
    "    import json\n",
    "    with open(os.path.join(out_folder, \"data.json\"), \"w\") as f:\n",
    "        json.dump(out_obj, f)\n",
    "generate_retrieval_image(2, \"/workspace/smart4.5/ignore/out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# visualizing label and retrieved molecules\n",
    "ds_out, ds_label = extract_sample(2)\n",
    "similarity = F.cosine_similarity(ds_out.unsqueeze(0), ds_label.unsqueeze(0))\n",
    "nz_pred, nz_label = ranker.normalized_to_nonzero(ds_out), ranker.normalized_to_nonzero(ds_label)\n",
    "\n",
    "original_smiles = ranker.lookup.get(nz_label, None)\n",
    "single = list(original_smiles)[0]\n",
    "\n",
    "results = ranker.retrieve(ds_out)\n",
    "rank = ranker.batched_rank(ds_out.unsqueeze(0), ds_label.unsqueeze(0))[0].item()\n",
    "print(\"Cossim of output and label\", similarity.item())\n",
    "print(\"Ground Truth:\", original_smiles)\n",
    "print(\"Rank:\", rank)\n",
    "\n",
    "plot_smiles(single)\n",
    "\n",
    "print(\"=== Retrieval ===\")\n",
    "for i, v in enumerate(results):\n",
    "    print(f\"*** Rank {i:3d} | {v} ***\")\n",
    "    first = list(v)[0]\n",
    "    plot_smiles(first)\n",
    "    \n",
    "    fig, sim = molDiff(list(original_smiles)[0], first)\n",
    "    display(fig)\n",
    "    print(\"*** ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
