{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file takes in colin's data.zip from 05/03 and adds hyunwoo's fingerprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, numpy as np, os, shutil, pickle, rdkit, sys, tqdm, fingerprint_utils, torch\n",
    "assert(os.path.exists(\"../../tempdata/data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating hyun_fp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shatter(shatter_dir, obj, id, keys):\n",
    "    '''\n",
    "        Saves an object as individual files\n",
    "    '''\n",
    "    for key in keys:\n",
    "        elem = obj[key]\n",
    "        if type(elem) == np.ndarray:\n",
    "            torch.save(torch.tensor(elem, dtype=torch.float), f\"{shatter_dir}/{key}/{id}.pt\")\n",
    "        elif type(elem) == str:\n",
    "            torch.save(elem, f\"{shatter_dir}/{key}/{id}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"../../tempdata/data/*/*.pkl\")\n",
    "out_dir = \"../../tempdata/hyun_fp_data\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "save_attrs = {\n",
    "    \"hsqc_ms_pairs\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"HSQC\", \"MS\"],\n",
    "    \"hsqc_pretrain\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"HSQC\"],\n",
    "    \"ms_pretrain\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"MS\"]\n",
    "}\n",
    "for file_path in files:\n",
    "    subdivision = file_path.split(\"/\")[-2]\n",
    "    split = file_path.split(\"/\")[-1]\n",
    "    split = split[:split.index(\".\")]\n",
    "\n",
    "    for my_attr in save_attrs[subdivision]:\n",
    "        os.makedirs(f\"{out_dir}/{subdivision}/{split}/{my_attr}\", exist_ok=True)\n",
    "\n",
    "    print(file_path, subdivision, split)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "\n",
    "    keys = set(obj.keys())\n",
    "    for k in keys:\n",
    "        v = obj[k]\n",
    "        v[\"HYUN_FP\"] = fingerprint_utils.FP_generator(v[\"SMILES\"], 2)\n",
    "        shatter(f\"{out_dir}/{subdivision}/{split}\", v, k, save_attrs[subdivision])\n",
    "        del obj[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(out_dir):\n",
    "    for g in os.listdir(os.path.join(out_dir, f)):\n",
    "        print(f\"{f}/{g}/{len(os.listdir(os.path.join(out_dir, f, g, 'FP')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Dataset using better bounds / hsqc intensity rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.randint(-10, 10, (30, 50, 3)).float()\n",
    "v[:,:,2] = torch.where(v[:,:,2] >= 0, 1.0, -1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tempdata/data/hsqc_ms_pairs/train.pkl hsqc_ms_pairs train\n",
      "['SMILES', 'FP', 'HYUN_FP', 'HSQC', 'MS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03:29:51] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tempdata/data/hsqc_ms_pairs/val.pkl hsqc_ms_pairs val\n",
      "['SMILES', 'FP', 'HYUN_FP', 'HSQC', 'MS']\n",
      "../../tempdata/data/hsqc_ms_pairs/test.pkl hsqc_ms_pairs test\n",
      "['SMILES', 'FP', 'HYUN_FP', 'HSQC', 'MS']\n",
      "../../tempdata/data/hsqc_pretrain/train.pkl hsqc_pretrain train\n",
      "['SMILES', 'FP', 'HYUN_FP', 'HSQC']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03:31:20] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:31:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:31:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:31:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:32:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:33:18] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:34:02] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:35:41] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:36:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:37:54] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tempdata/data/hsqc_pretrain/val.pkl hsqc_pretrain val\n",
      "['SMILES', 'FP', 'HYUN_FP', 'HSQC']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03:39:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:39:30] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tempdata/data/hsqc_pretrain/test.pkl hsqc_pretrain test\n",
      "['SMILES', 'FP', 'HYUN_FP', 'HSQC']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03:40:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:40:11] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tempdata/data/ms_pretrain/train.pkl ms_pretrain train\n",
      "['SMILES', 'FP', 'HYUN_FP', 'MS']\n",
      "../../tempdata/data/ms_pretrain/val.pkl ms_pretrain val\n",
      "['SMILES', 'FP', 'HYUN_FP', 'MS']\n",
      "../../tempdata/data/ms_pretrain/test.pkl ms_pretrain test\n",
      "['SMILES', 'FP', 'HYUN_FP', 'MS']\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(\"../../tempdata/data/*/*.pkl\")\n",
    "out_dir = \"../../tempdata/bounded_hyun_fp_data\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "save_attrs = {\n",
    "    \"hsqc_ms_pairs\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"HSQC\", \"MS\"],\n",
    "    \"hsqc_pretrain\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"HSQC\"],\n",
    "    \"ms_pretrain\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"MS\"]\n",
    "}\n",
    "\n",
    "for file_path in files:\n",
    "    subdivision = file_path.split(\"/\")[-2]\n",
    "    split = file_path.split(\"/\")[-1]\n",
    "    split = split[:split.index(\".\")]\n",
    "\n",
    "    for my_attr in save_attrs[subdivision]:\n",
    "        os.makedirs(f\"{out_dir}/{subdivision}/{split}/{my_attr}\", exist_ok=True)\n",
    "\n",
    "    print(file_path, subdivision, split)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "\n",
    "    keys = set(obj.keys())\n",
    "    print(save_attrs[subdivision])\n",
    "    for k in keys:\n",
    "        v = obj[k]\n",
    "        v[\"HYUN_FP\"] = fingerprint_utils.FP_generator(v[\"SMILES\"], 2)\n",
    "        shatter2(f\"{out_dir}/{subdivision}/{split}\", v, k, save_attrs[subdivision])\n",
    "        del obj[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsqc_ms_pairs/train/1\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir(out_dir):\n",
    "    for g in os.listdir(os.path.join(out_dir, f)):\n",
    "        print(f\"{f}/{g}/{len(os.listdir(os.path.join(out_dir, f, g, 'FP')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating same dataset, but using hsqc as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @wangdong make sure you pull colin_split_05_23_22.zip before running this code\n",
    "# this creates the pickle files into ../../tempdata/data/...\n",
    "def shatter(shatter_dir, obj, id, keys):\n",
    "    '''\n",
    "        Saves an object as individual files\n",
    "    '''\n",
    "    for key in keys:\n",
    "        elem = obj[key]\n",
    "        if type(elem) == np.ndarray:\n",
    "            if key == \"HSQC\":\n",
    "                # @wangdong you can add handler functions here to pre-process your data\n",
    "                elem2 = elem\n",
    "                torch.save(elem2, f\"{shatter_dir}/{key}/{id}.pt\")\n",
    "                pass\n",
    "            else:\n",
    "                torch.save(torch.tensor(elem, dtype=torch.float), f\"{shatter_dir}/{key}/{id}.pt\")\n",
    "        elif type(elem) == str:\n",
    "            torch.save(elem, f\"{shatter_dir}/{key}/{id}.pt\")\n",
    "\n",
    "files = glob.glob(\"../../tempdata/data/*/*.pkl\") # colin's input\n",
    "output_dir_name = None # @wandong change the output directory here\n",
    "assert(output_dir_name)\n",
    "out_dir = f\"../../tempdata/{output_dir_name}\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "save_attrs = {\n",
    "    \"hsqc_ms_pairs\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"HSQC\", \"MS\"],\n",
    "    \"hsqc_pretrain\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"HSQC\"],\n",
    "    \"ms_pretrain\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"MS\"]\n",
    "}\n",
    "for file_path in files:\n",
    "    subdivision = file_path.split(\"/\")[-2]\n",
    "    split = file_path.split(\"/\")[-1]\n",
    "    split = split[:split.index(\".\")]\n",
    "\n",
    "    for my_attr in save_attrs[subdivision]:\n",
    "        os.makedirs(f\"{out_dir}/{subdivision}/{split}/{my_attr}\", exist_ok=True)\n",
    "\n",
    "    print(file_path, subdivision, split)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "\n",
    "    keys = set(obj.keys())\n",
    "    for k in keys:\n",
    "        v = obj[k] # the object that you want to split into directories\n",
    "        v[\"HYUN_FP\"] = fingerprint_utils.FP_generator(v[\"SMILES\"], 2)\n",
    "        shatter(f\"{out_dir}/{subdivision}/{split}\", v, k, save_attrs[subdivision])\n",
    "        del obj[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10/25/22: We were advised from 10/20 NIH meeting to only use 10 MS specs. This creates a new dataset using only top 10 specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1    6]\n",
      " [   2    4]\n",
      " [   3    9]\n",
      " [  -1   -4]\n",
      " [  -2    0]\n",
      " [-899 -999]\n",
      " [-999  -55]]\n",
      "[[4 5]\n",
      " [6 6]\n",
      " [5 3]\n",
      " [3 4]\n",
      " [0 1]\n",
      " [1 2]\n",
      " [2 0]]\n",
      "[[-899 -999]\n",
      " [-999  -55]\n",
      " [  -1   -4]\n",
      " [  -2    0]\n",
      " [   2    4]\n",
      " [   3    9]\n",
      " [   1    6]]\n"
     ]
    }
   ],
   "source": [
    "d = np.array([[1, 6], [2, 4], [3, 9], [-1, -4], [-2, 0], [-899, -999], [-999, -55]])\n",
    "print(d)\n",
    "print(np.argpartition(d, -3, 0))\n",
    "print(d[np.argpartition(d[:,1], -3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shatter_small_ms(shatter_dir, obj, id, keys):\n",
    "    '''\n",
    "        Saves an object as individual files\n",
    "\n",
    "        <dataset_type>\n",
    "            - <split>\n",
    "                - <key>\n",
    "                    - id.pt\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        shatter_dir: directory to place file into. <dataset_type>/<split>\n",
    "        obj: the object to split\n",
    "        id: the id that colin assigned to the sample\n",
    "        keys: the feature directories to assign\n",
    "    '''\n",
    "    for key in keys: # FP, SMILES, etc.\n",
    "        elem = obj[key]\n",
    "        if key == \"MS\" and len(elem) > 10:\n",
    "            # gets indexes of 10 highest items by intensity. Everything from [-10:,1] will be indexes\n",
    "            # of things with a higher intensity than the 10'th highest intensity\n",
    "            partition = np.argpartition(elem, -10, axis=0)[-10:,1]\n",
    "            elem = elem[partition]\n",
    "\n",
    "        if type(elem) == np.ndarray:\n",
    "            torch.save(torch.tensor(elem, dtype=torch.float), f\"{shatter_dir}/{key}/{id}.pt\")\n",
    "        elif type(elem) == str:\n",
    "            torch.save(elem, f\"{shatter_dir}/{key}/{id}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tempdata/data/hsqc_ms_pairs/train.pkl hsqc_ms_pairs train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[06:50:54] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tempdata/data/hsqc_ms_pairs/val.pkl hsqc_ms_pairs val\n",
      "../../tempdata/data/hsqc_ms_pairs/test.pkl hsqc_ms_pairs test\n",
      "../../tempdata/data/hsqc_pretrain/train.pkl hsqc_pretrain train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[06:52:13] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:52:19] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:52:22] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:52:38] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:52:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:53:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:54:19] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:55:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:56:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:57:18] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tempdata/data/hsqc_pretrain/val.pkl hsqc_pretrain val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[06:58:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:58:46] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tempdata/data/hsqc_pretrain/test.pkl hsqc_pretrain test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[06:59:31] WARNING: not removing hydrogen atom without neighbors\n",
      "[06:59:31] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tempdata/data/ms_pretrain/train.pkl ms_pretrain train\n",
      "../../tempdata/data/ms_pretrain/val.pkl ms_pretrain val\n",
      "../../tempdata/data/ms_pretrain/test.pkl ms_pretrain test\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(\"../../tempdata/data/*/*.pkl\") # glob files from colin_split_05_03_22\n",
    "out_dir = \"../../tempdata/smaller_ms\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "save_attrs = {\n",
    "    \"hsqc_ms_pairs\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"HSQC\", \"MS\"],\n",
    "    \"hsqc_pretrain\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"HSQC\"],\n",
    "    \"ms_pretrain\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"MS\"]\n",
    "}\n",
    "for file_path in files:\n",
    "    subdivision = file_path.split(\"/\")[-2] # one of hsqc_ms_pairs, hsqc_pretrain, ms_pretrain\n",
    "    split = file_path.split(\"/\")[-1] # one of train, val, test\n",
    "    split = split[:split.index(\".\")]\n",
    "\n",
    "    for my_attr in save_attrs[subdivision]:\n",
    "        os.makedirs(f\"{out_dir}/{subdivision}/{split}/{my_attr}\", exist_ok=True)\n",
    "\n",
    "    print(file_path, subdivision, split)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "\n",
    "    keys = set(obj.keys()) # list of id's\n",
    "    for k in keys:\n",
    "        v = obj[k]\n",
    "        v[\"HYUN_FP\"] = fingerprint_utils.FP_generator(v[\"SMILES\"], 2)\n",
    "        shatter_small_ms(f\"{out_dir}/{subdivision}/{split}\", v, k, save_attrs[subdivision])\n",
    "        del obj[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
