{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file takes in colin's data.zip from 05/03 and adds hyunwoo's fingerprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, numpy as np, os, shutil, pickle, rdkit, sys, tqdm, fingerprint_utils, torch\n",
    "\n",
    "assert(os.path.exists(\"../../tempdata/data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating hyun_fp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shatter(shatter_dir, obj, id, keys):\n",
    "    '''\n",
    "        Saves an object as individual files\n",
    "    '''\n",
    "    for key in keys:\n",
    "        elem = obj[key]\n",
    "        if type(elem) == np.ndarray:\n",
    "            torch.save(torch.tensor(elem, dtype=torch.float), f\"{shatter_dir}/{key}/{id}.pt\")\n",
    "        elif type(elem) == str:\n",
    "            torch.save(elem, f\"{shatter_dir}/{key}/{id}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"../../tempdata/data/*/*.pkl\")\n",
    "out_dir = \"../../tempdata/hyun_fp_data\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "save_attrs = {\n",
    "    \"hsqc_ms_pairs\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"HSQC\", \"MS\"],\n",
    "    \"hsqc_pretrain\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"HSQC\"],\n",
    "    \"ms_pretrain\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"MS\"]\n",
    "}\n",
    "for file_path in files:\n",
    "    subdivision = file_path.split(\"/\")[-2]\n",
    "    split = file_path.split(\"/\")[-1]\n",
    "    split = split[:split.index(\".\")]\n",
    "\n",
    "    for my_attr in save_attrs[subdivision]:\n",
    "        os.makedirs(f\"{out_dir}/{subdivision}/{split}/{my_attr}\", exist_ok=True)\n",
    "\n",
    "    print(file_path, subdivision, split)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "\n",
    "    keys = set(obj.keys())\n",
    "    for k in keys:\n",
    "        v = obj[k]\n",
    "        v[\"HYUN_FP\"] = fingerprint_utils.FP_generator(v[\"SMILES\"], 2)\n",
    "        shatter(f\"{out_dir}/{subdivision}/{split}\", v, k, save_attrs[subdivision])\n",
    "        del obj[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(out_dir):\n",
    "    for g in os.listdir(os.path.join(out_dir, f)):\n",
    "        print(f\"{f}/{g}/{len(os.listdir(os.path.join(out_dir, f, g, 'FP')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Dataset using better bounds / hsqc intensity rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  6.,  -4.,  -8.,  ...,  -7.,  -7.,  -6.],\n",
      "        [ -6.,   1.,   9.,  ...,  -8.,  -8.,  -6.],\n",
      "        [  3., -10., -10.,  ...,  -6.,   1.,   9.],\n",
      "        ...,\n",
      "        [ -2.,   3.,  -6.,  ...,   4.,   9.,   7.],\n",
      "        [ -1.,   8.,  -6.,  ...,   7., -10.,  -7.],\n",
      "        [ -7.,   8.,  -2.,  ...,   6.,  -6.,   3.]])\n",
      "tensor([[ 1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "        [ 1., -1., -1.,  ..., -1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-1.,  1., -1.,  ...,  1.,  1.,  1.],\n",
      "        [-1.,  1., -1.,  ...,  1., -1., -1.],\n",
      "        [-1.,  1., -1.,  ...,  1., -1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "v = torch.randint(-10, 10, (30, 50, 3)).float()\n",
    "v[:,:,2] = torch.where(v[:,:,2] >= 0, 1.0, -1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def shatter2(shatter_dir, obj, id, keys):\n",
    "    '''\n",
    "        Saves an object as individual files\n",
    "    '''\n",
    "    for key in keys:\n",
    "        elem = obj[key]\n",
    "        if type(elem) == np.ndarray:\n",
    "            if key == \"HSQC\":\n",
    "                elem[:,2] = np.where(elem[:,2] >= 0, 1.0, -1.0)\n",
    "            torch.save(torch.tensor(elem, dtype=torch.float), f\"{shatter_dir}/{key}/{id}.pt\")\n",
    "        elif type(elem) == str:\n",
    "            torch.save(elem, f\"{shatter_dir}/{key}/{id}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tempdata/data/hsqc_ms_pairs/train.pkl hsqc_ms_pairs train\n",
      "['SMILES', 'FP', 'HYUN_FP', 'HSQC', 'MS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03:29:51] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tempdata/data/hsqc_ms_pairs/val.pkl hsqc_ms_pairs val\n",
      "['SMILES', 'FP', 'HYUN_FP', 'HSQC', 'MS']\n",
      "../../tempdata/data/hsqc_ms_pairs/test.pkl hsqc_ms_pairs test\n",
      "['SMILES', 'FP', 'HYUN_FP', 'HSQC', 'MS']\n",
      "../../tempdata/data/hsqc_pretrain/train.pkl hsqc_pretrain train\n",
      "['SMILES', 'FP', 'HYUN_FP', 'HSQC']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03:31:20] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:31:28] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:31:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:31:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:32:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:33:18] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:34:02] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:35:41] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:36:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:37:54] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tempdata/data/hsqc_pretrain/val.pkl hsqc_pretrain val\n",
      "['SMILES', 'FP', 'HYUN_FP', 'HSQC']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03:39:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:39:30] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tempdata/data/hsqc_pretrain/test.pkl hsqc_pretrain test\n",
      "['SMILES', 'FP', 'HYUN_FP', 'HSQC']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03:40:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[03:40:11] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tempdata/data/ms_pretrain/train.pkl ms_pretrain train\n",
      "['SMILES', 'FP', 'HYUN_FP', 'MS']\n",
      "../../tempdata/data/ms_pretrain/val.pkl ms_pretrain val\n",
      "['SMILES', 'FP', 'HYUN_FP', 'MS']\n",
      "../../tempdata/data/ms_pretrain/test.pkl ms_pretrain test\n",
      "['SMILES', 'FP', 'HYUN_FP', 'MS']\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(\"../../tempdata/data/*/*.pkl\")\n",
    "out_dir = \"../../tempdata/bounded_hyun_fp_data\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "save_attrs = {\n",
    "    \"hsqc_ms_pairs\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"HSQC\", \"MS\"],\n",
    "    \"hsqc_pretrain\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"HSQC\"],\n",
    "    \"ms_pretrain\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"MS\"]\n",
    "}\n",
    "\n",
    "for file_path in files:\n",
    "    subdivision = file_path.split(\"/\")[-2]\n",
    "    split = file_path.split(\"/\")[-1]\n",
    "    split = split[:split.index(\".\")]\n",
    "\n",
    "    for my_attr in save_attrs[subdivision]:\n",
    "        os.makedirs(f\"{out_dir}/{subdivision}/{split}/{my_attr}\", exist_ok=True)\n",
    "\n",
    "    print(file_path, subdivision, split)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "\n",
    "    keys = set(obj.keys())\n",
    "    print(save_attrs[subdivision])\n",
    "    for k in keys:\n",
    "        v = obj[k]\n",
    "        v[\"HYUN_FP\"] = fingerprint_utils.FP_generator(v[\"SMILES\"], 2)\n",
    "        shatter2(f\"{out_dir}/{subdivision}/{split}\", v, k, save_attrs[subdivision])\n",
    "        del obj[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsqc_ms_pairs/train/1\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir(out_dir):\n",
    "    for g in os.listdir(os.path.join(out_dir, f)):\n",
    "        print(f\"{f}/{g}/{len(os.listdir(os.path.join(out_dir, f, g, 'FP')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating same dataset, but using hsqc as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @wangdong make sure you pull colin_split_05_23_22.zip before running this code\n",
    "# this creates the pickle files into ../../tempdata/data/...\n",
    "def shatter(shatter_dir, obj, id, keys):\n",
    "    '''\n",
    "        Saves an object as individual files\n",
    "    '''\n",
    "    for key in keys:\n",
    "        elem = obj[key]\n",
    "        if type(elem) == np.ndarray:\n",
    "            if key == \"HSQC\":\n",
    "                # @wangdong you can add handler functions here to pre-process your data\n",
    "                elem2 = elem\n",
    "                torch.save(elem2, f\"{shatter_dir}/{key}/{id}.pt\")\n",
    "                pass\n",
    "            else:\n",
    "                torch.save(torch.tensor(elem, dtype=torch.float), f\"{shatter_dir}/{key}/{id}.pt\")\n",
    "        elif type(elem) == str:\n",
    "            torch.save(elem, f\"{shatter_dir}/{key}/{id}.pt\")\n",
    "\n",
    "files = glob.glob(\"../../tempdata/data/*/*.pkl\") # colin's input\n",
    "output_dir_name = None # @wandong change the output directory here\n",
    "assert(output_dir_name)\n",
    "out_dir = f\"../../tempdata/{output_dir_name}\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "save_attrs = {\n",
    "    \"hsqc_ms_pairs\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"HSQC\", \"MS\"],\n",
    "    \"hsqc_pretrain\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"HSQC\"],\n",
    "    \"ms_pretrain\": [\"SMILES\", \"FP\", \"HYUN_FP\", \"MS\"]\n",
    "}\n",
    "for file_path in files:\n",
    "    subdivision = file_path.split(\"/\")[-2]\n",
    "    split = file_path.split(\"/\")[-1]\n",
    "    split = split[:split.index(\".\")]\n",
    "\n",
    "    for my_attr in save_attrs[subdivision]:\n",
    "        os.makedirs(f\"{out_dir}/{subdivision}/{split}/{my_attr}\", exist_ok=True)\n",
    "\n",
    "    print(file_path, subdivision, split)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "\n",
    "    keys = set(obj.keys())\n",
    "    for k in keys:\n",
    "        v = obj[k] # the object that you want to split into directories\n",
    "        v[\"HYUN_FP\"] = fingerprint_utils.FP_generator(v[\"SMILES\"], 2)\n",
    "        shatter(f\"{out_dir}/{subdivision}/{split}\", v, k, save_attrs[subdivision])\n",
    "        del obj[k]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
