{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05/12/2023\n",
    "\n",
    "The purpose of this notebook is to test the SMART_CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from datasets.dataset_utils import pad_and_mask, tokenise_and_mask_encoder\n",
    "from datasets.generic_index_dataset import GenericIndexedModule\n",
    "from models.chemformer.tokeniser import MolEncTokeniser\n",
    "from models.chemformer.utils import REGEX\n",
    "\n",
    "from models.smart_clip import SMART_CLIP\n",
    "\n",
    "path = \"./tempdata/epoch=76-step=264264.ckpt\"\n",
    "vocab_path = \"tempdata/chemformer/bart_vocab.txt\"\n",
    "chem_token_start = 272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = MolEncTokeniser.from_vocab_file(\n",
    "  vocab_path, REGEX, chem_token_start\n",
    ")\n",
    "direct = Path(\"tempdata/SMILES_dataset\")\n",
    "features = [\"HSQC\", \"SMILES\"]\n",
    "def tame(a):\n",
    "  return tokenise_and_mask_encoder(a, tokeniser)\n",
    "feature_handlers = [pad_and_mask, tame]\n",
    "gim = GenericIndexedModule(direct, features, feature_handlers, len_override = 5)\n",
    "gim.setup(\"fit\")\n",
    "train_dl = gim.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence torch.Size([5, 21, 3])\n",
      "padding_mask torch.Size([5, 21])\n",
      "encoder_inputs torch.Size([5, 41])\n",
      "encoder_mask torch.Size([5, 41])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dl:\n",
    "  obj = batch\n",
    "  for item in obj:\n",
    "    for k in item:\n",
    "      if type(item[k]) is torch.Tensor:\n",
    "        print(k, item[k].size())\n",
    "        item[k] = item[k].cuda()\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/migration/migration.py:201: PossibleUserWarning: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "  rank_zero_warn(\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.2.3 to v2.0.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file tempdata/chemformer/model.ckpt`\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:157: UserWarning: Found keys that are not in the model state dict but in the checkpoint: ['decoder.layers.0.self_attn.in_proj_weight', 'decoder.layers.0.self_attn.in_proj_bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.multihead_attn.in_proj_weight', 'decoder.layers.0.multihead_attn.in_proj_bias', 'decoder.layers.0.multihead_attn.out_proj.weight', 'decoder.layers.0.multihead_attn.out_proj.bias', 'decoder.layers.0.linear1.weight', 'decoder.layers.0.linear1.bias', 'decoder.layers.0.linear2.weight', 'decoder.layers.0.linear2.bias', 'decoder.layers.0.norm1.weight', 'decoder.layers.0.norm1.bias', 'decoder.layers.0.norm2.weight', 'decoder.layers.0.norm2.bias', 'decoder.layers.0.norm3.weight', 'decoder.layers.0.norm3.bias', 'decoder.layers.1.self_attn.in_proj_weight', 'decoder.layers.1.self_attn.in_proj_bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.multihead_attn.in_proj_weight', 'decoder.layers.1.multihead_attn.in_proj_bias', 'decoder.layers.1.multihead_attn.out_proj.weight', 'decoder.layers.1.multihead_attn.out_proj.bias', 'decoder.layers.1.linear1.weight', 'decoder.layers.1.linear1.bias', 'decoder.layers.1.linear2.weight', 'decoder.layers.1.linear2.bias', 'decoder.layers.1.norm1.weight', 'decoder.layers.1.norm1.bias', 'decoder.layers.1.norm2.weight', 'decoder.layers.1.norm2.bias', 'decoder.layers.1.norm3.weight', 'decoder.layers.1.norm3.bias', 'decoder.layers.2.self_attn.in_proj_weight', 'decoder.layers.2.self_attn.in_proj_bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.multihead_attn.in_proj_weight', 'decoder.layers.2.multihead_attn.in_proj_bias', 'decoder.layers.2.multihead_attn.out_proj.weight', 'decoder.layers.2.multihead_attn.out_proj.bias', 'decoder.layers.2.linear1.weight', 'decoder.layers.2.linear1.bias', 'decoder.layers.2.linear2.weight', 'decoder.layers.2.linear2.bias', 'decoder.layers.2.norm1.weight', 'decoder.layers.2.norm1.bias', 'decoder.layers.2.norm2.weight', 'decoder.layers.2.norm2.bias', 'decoder.layers.2.norm3.weight', 'decoder.layers.2.norm3.bias', 'decoder.layers.3.self_attn.in_proj_weight', 'decoder.layers.3.self_attn.in_proj_bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.multihead_attn.in_proj_weight', 'decoder.layers.3.multihead_attn.in_proj_bias', 'decoder.layers.3.multihead_attn.out_proj.weight', 'decoder.layers.3.multihead_attn.out_proj.bias', 'decoder.layers.3.linear1.weight', 'decoder.layers.3.linear1.bias', 'decoder.layers.3.linear2.weight', 'decoder.layers.3.linear2.bias', 'decoder.layers.3.norm1.weight', 'decoder.layers.3.norm1.bias', 'decoder.layers.3.norm2.weight', 'decoder.layers.3.norm2.bias', 'decoder.layers.3.norm3.weight', 'decoder.layers.3.norm3.bias', 'decoder.layers.4.self_attn.in_proj_weight', 'decoder.layers.4.self_attn.in_proj_bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.multihead_attn.in_proj_weight', 'decoder.layers.4.multihead_attn.in_proj_bias', 'decoder.layers.4.multihead_attn.out_proj.weight', 'decoder.layers.4.multihead_attn.out_proj.bias', 'decoder.layers.4.linear1.weight', 'decoder.layers.4.linear1.bias', 'decoder.layers.4.linear2.weight', 'decoder.layers.4.linear2.bias', 'decoder.layers.4.norm1.weight', 'decoder.layers.4.norm1.bias', 'decoder.layers.4.norm2.weight', 'decoder.layers.4.norm2.bias', 'decoder.layers.4.norm3.weight', 'decoder.layers.4.norm3.bias', 'decoder.layers.5.self_attn.in_proj_weight', 'decoder.layers.5.self_attn.in_proj_bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.multihead_attn.in_proj_weight', 'decoder.layers.5.multihead_attn.in_proj_bias', 'decoder.layers.5.multihead_attn.out_proj.weight', 'decoder.layers.5.multihead_attn.out_proj.bias', 'decoder.layers.5.linear1.weight', 'decoder.layers.5.linear1.bias', 'decoder.layers.5.linear2.weight', 'decoder.layers.5.linear2.bias', 'decoder.layers.5.norm1.weight', 'decoder.layers.5.norm1.bias', 'decoder.layers.5.norm2.weight', 'decoder.layers.5.norm2.bias', 'decoder.layers.5.norm3.weight', 'decoder.layers.5.norm3.bias', 'decoder.norm.weight', 'decoder.norm.bias', 'token_fc.weight', 'token_fc.bias']\n",
      "  rank_zero_warn(\n",
      "Initialized SignCoordinateEncoder[128] with dims [43, 43, 42] and 2 positional encoders. 42 bits are reserved for encoding the final bit\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "  \"chemformer_path\": \"tempdata/chemformer/model.ckpt\",\n",
    "  \"projection_dim\": 256,\n",
    "  \"lr\": 1e-5,\n",
    "  \"coord_enc\": \"sce\",\n",
    "  \"enc_args\": {\n",
    "    \"wavelength_bounds\": ((0.01, 150), (0.01, 150))\n",
    "  },\n",
    "  \"dim_model\": 128\n",
    "}\n",
    "model = SMART_CLIP(**kwargs).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsqc_out.size()=torch.Size([5, 128])\n",
      "chemformer_out.size()=torch.Size([5, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsqc, smiles = model(obj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
