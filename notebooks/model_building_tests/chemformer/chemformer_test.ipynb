{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04/14/23\n",
    "\n",
    "This notebook is intended to test chemformer loading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from models.chemformer.molbart import BARTModel\n",
    "from models.chemformer.tokeniser import MolEncTokeniser\n",
    "from models.chemformer.utils import REGEX, DEFAULT_MAX_SEQ_LEN\n",
    "from models.chemformer.molbart_dataset import Uspto50\n",
    "from models.chemformer.molbart_datamodule import FineTuneReactionDataModule\n",
    "from models.chemformer.decoder import DecodeSampler\n",
    "\n",
    "from datasets.dataset_utils import pad, tokenise_and_mask\n",
    "from datasets.generic_index_dataset import GenericIndexedModule\n",
    "\n",
    "from models.ranked_transformer import Moonshot\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model. Using code from the original chemformer repo (```molbart/util.py```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary\n",
    "from pathlib import Path\n",
    "vocab_path = \"tempdata/chemformer/bart_vocab.txt\"\n",
    "chem_token_start = 272\n",
    "tokeniser = MolEncTokeniser.from_vocab_file(\n",
    "  vocab_path, REGEX, chem_token_start\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_pad_masks': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]],\n",
       " 'masked_pad_masks': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]],\n",
       " 'original_tokens': [['^',\n",
       "   'C',\n",
       "   'O',\n",
       "   'C',\n",
       "   '(',\n",
       "   '=',\n",
       "   'O',\n",
       "   ')',\n",
       "   'C',\n",
       "   'C',\n",
       "   'C',\n",
       "   '(',\n",
       "   '=',\n",
       "   'O',\n",
       "   ')',\n",
       "   'c',\n",
       "   '1',\n",
       "   'c',\n",
       "   'c',\n",
       "   'c',\n",
       "   '(',\n",
       "   'O',\n",
       "   'C',\n",
       "   '2',\n",
       "   'C',\n",
       "   'C',\n",
       "   'C',\n",
       "   'C',\n",
       "   'O',\n",
       "   '2',\n",
       "   ')',\n",
       "   'c',\n",
       "   'c',\n",
       "   '1',\n",
       "   'O',\n",
       "   '&'],\n",
       "  ['^',\n",
       "   'C',\n",
       "   'O',\n",
       "   'O',\n",
       "   'H',\n",
       "   '&',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>',\n",
       "   '<PAD>']]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.tokenise([\"COC(=O)CCC(=O)c1ccc(OC2CCCCO2)cc1O\", \"COOH\"], pad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uspto50 __init()__: \n",
      "[DS] <class 'pandas.core.frame.DataFrame'> 50037\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "data_path = \"tempdata/chemformer/uspto_50.pickle\"\n",
    "aug_prob = 0.0\n",
    "dataset = Uspto50(\n",
    "  data_path, aug_prob, forward=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a batch size of 2.\n",
      "Building data module for forward prediction task...\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model_type = \"bart\"\n",
    "uni_model = model_type == \"unified\"\n",
    "batch_size = 2\n",
    "train_tokens = None\n",
    "num_buckets = None\n",
    "\n",
    "dm = FineTuneReactionDataModule(\n",
    "    dataset,\n",
    "    tokeniser,\n",
    "    batch_size,\n",
    "    DEFAULT_MAX_SEQ_LEN,\n",
    "    forward_pred=True,\n",
    "    val_idxs=dataset.val_idxs,\n",
    "    test_idxs=dataset.test_idxs,\n",
    "    train_token_batch_size=train_tokens,\n",
    "    num_buckets=num_buckets,\n",
    "    unified_model=uni_model\n",
    ")\n",
    "sampler = DecodeSampler(tokeniser, DEFAULT_MAX_SEQ_LEN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chemformer Hacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_path = \"tempdata/chemformer/model.ckpt\"\n",
    "obj = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = Path(\"tempdata/SMILES_dataset\")\n",
    "features = [\"HSQC\", \"SMILES\"]\n",
    "feature_handlers = [pad, tokenise_and_mask]\n",
    "gim = GenericIndexedModule(direct, features, feature_handlers, len_override = 5, molbart_tokeniser = tokeniser)\n",
    "gim.setup(\"fit\")\n",
    "train_dl = gim.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/migration/migration.py:201: PossibleUserWarning: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "  rank_zero_warn(\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.2.3 to v2.0.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file tempdata/chemformer/model.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed an encoder with no defined bounds\n",
      "Pushed an encoder with no defined bounds\n",
      "Pushed an encoder with no defined bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:153: UserWarning: Found keys that are in the model state dict but not in the checkpoint: ['latent', 'fc.weight', 'fc.bias', 'transformer_encoder.layers.0.self_attn.in_proj_weight', 'transformer_encoder.layers.0.self_attn.in_proj_bias', 'transformer_encoder.layers.0.self_attn.out_proj.weight', 'transformer_encoder.layers.0.self_attn.out_proj.bias', 'transformer_encoder.layers.0.linear1.weight', 'transformer_encoder.layers.0.linear1.bias', 'transformer_encoder.layers.0.linear2.weight', 'transformer_encoder.layers.0.linear2.bias', 'transformer_encoder.layers.0.norm1.weight', 'transformer_encoder.layers.0.norm1.bias', 'transformer_encoder.layers.0.norm2.weight', 'transformer_encoder.layers.0.norm2.bias', 'transformer_encoder.layers.1.self_attn.in_proj_weight', 'transformer_encoder.layers.1.self_attn.in_proj_bias', 'transformer_encoder.layers.1.self_attn.out_proj.weight', 'transformer_encoder.layers.1.self_attn.out_proj.bias', 'transformer_encoder.layers.1.linear1.weight', 'transformer_encoder.layers.1.linear1.bias', 'transformer_encoder.layers.1.linear2.weight', 'transformer_encoder.layers.1.linear2.bias', 'transformer_encoder.layers.1.norm1.weight', 'transformer_encoder.layers.1.norm1.bias', 'transformer_encoder.layers.1.norm2.weight', 'transformer_encoder.layers.1.norm2.bias', 'transformer_encoder.layers.2.self_attn.in_proj_weight', 'transformer_encoder.layers.2.self_attn.in_proj_bias', 'transformer_encoder.layers.2.self_attn.out_proj.weight', 'transformer_encoder.layers.2.self_attn.out_proj.bias', 'transformer_encoder.layers.2.linear1.weight', 'transformer_encoder.layers.2.linear1.bias', 'transformer_encoder.layers.2.linear2.weight', 'transformer_encoder.layers.2.linear2.bias', 'transformer_encoder.layers.2.norm1.weight', 'transformer_encoder.layers.2.norm1.bias', 'transformer_encoder.layers.2.norm2.weight', 'transformer_encoder.layers.2.norm2.bias', 'transformer_encoder.layers.3.self_attn.in_proj_weight', 'transformer_encoder.layers.3.self_attn.in_proj_bias', 'transformer_encoder.layers.3.self_attn.out_proj.weight', 'transformer_encoder.layers.3.self_attn.out_proj.bias', 'transformer_encoder.layers.3.linear1.weight', 'transformer_encoder.layers.3.linear1.bias', 'transformer_encoder.layers.3.linear2.weight', 'transformer_encoder.layers.3.linear2.bias', 'transformer_encoder.layers.3.norm1.weight', 'transformer_encoder.layers.3.norm1.bias', 'transformer_encoder.layers.3.norm2.weight', 'transformer_encoder.layers.3.norm2.bias', 'transformer_encoder.layers.4.self_attn.in_proj_weight', 'transformer_encoder.layers.4.self_attn.in_proj_bias', 'transformer_encoder.layers.4.self_attn.out_proj.weight', 'transformer_encoder.layers.4.self_attn.out_proj.bias', 'transformer_encoder.layers.4.linear1.weight', 'transformer_encoder.layers.4.linear1.bias', 'transformer_encoder.layers.4.linear2.weight', 'transformer_encoder.layers.4.linear2.bias', 'transformer_encoder.layers.4.norm1.weight', 'transformer_encoder.layers.4.norm1.bias', 'transformer_encoder.layers.4.norm2.weight', 'transformer_encoder.layers.4.norm2.bias', 'transformer_encoder.layers.5.self_attn.in_proj_weight', 'transformer_encoder.layers.5.self_attn.in_proj_bias', 'transformer_encoder.layers.5.self_attn.out_proj.weight', 'transformer_encoder.layers.5.self_attn.out_proj.bias', 'transformer_encoder.layers.5.linear1.weight', 'transformer_encoder.layers.5.linear1.bias', 'transformer_encoder.layers.5.linear2.weight', 'transformer_encoder.layers.5.linear2.bias', 'transformer_encoder.layers.5.norm1.weight', 'transformer_encoder.layers.5.norm1.bias', 'transformer_encoder.layers.5.norm2.weight', 'transformer_encoder.layers.5.norm2.bias', 'transformer_encoder.layers.6.self_attn.in_proj_weight', 'transformer_encoder.layers.6.self_attn.in_proj_bias', 'transformer_encoder.layers.6.self_attn.out_proj.weight', 'transformer_encoder.layers.6.self_attn.out_proj.bias', 'transformer_encoder.layers.6.linear1.weight', 'transformer_encoder.layers.6.linear1.bias', 'transformer_encoder.layers.6.linear2.weight', 'transformer_encoder.layers.6.linear2.bias', 'transformer_encoder.layers.6.norm1.weight', 'transformer_encoder.layers.6.norm1.bias', 'transformer_encoder.layers.6.norm2.weight', 'transformer_encoder.layers.6.norm2.bias', 'transformer_encoder.layers.7.self_attn.in_proj_weight', 'transformer_encoder.layers.7.self_attn.in_proj_bias', 'transformer_encoder.layers.7.self_attn.out_proj.weight', 'transformer_encoder.layers.7.self_attn.out_proj.bias', 'transformer_encoder.layers.7.linear1.weight', 'transformer_encoder.layers.7.linear1.bias', 'transformer_encoder.layers.7.linear2.weight', 'transformer_encoder.layers.7.linear2.bias', 'transformer_encoder.layers.7.norm1.weight', 'transformer_encoder.layers.7.norm1.bias', 'transformer_encoder.layers.7.norm2.weight', 'transformer_encoder.layers.7.norm2.bias']\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:157: UserWarning: Found keys that are not in the model state dict but in the checkpoint: ['pos_emb', 'encoder.layers.0.self_attn.in_proj_weight', 'encoder.layers.0.self_attn.in_proj_bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.linear1.weight', 'encoder.layers.0.linear1.bias', 'encoder.layers.0.linear2.weight', 'encoder.layers.0.linear2.bias', 'encoder.layers.0.norm1.weight', 'encoder.layers.0.norm1.bias', 'encoder.layers.0.norm2.weight', 'encoder.layers.0.norm2.bias', 'encoder.layers.1.self_attn.in_proj_weight', 'encoder.layers.1.self_attn.in_proj_bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.linear1.weight', 'encoder.layers.1.linear1.bias', 'encoder.layers.1.linear2.weight', 'encoder.layers.1.linear2.bias', 'encoder.layers.1.norm1.weight', 'encoder.layers.1.norm1.bias', 'encoder.layers.1.norm2.weight', 'encoder.layers.1.norm2.bias', 'encoder.layers.2.self_attn.in_proj_weight', 'encoder.layers.2.self_attn.in_proj_bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.linear1.weight', 'encoder.layers.2.linear1.bias', 'encoder.layers.2.linear2.weight', 'encoder.layers.2.linear2.bias', 'encoder.layers.2.norm1.weight', 'encoder.layers.2.norm1.bias', 'encoder.layers.2.norm2.weight', 'encoder.layers.2.norm2.bias', 'encoder.layers.3.self_attn.in_proj_weight', 'encoder.layers.3.self_attn.in_proj_bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.linear1.weight', 'encoder.layers.3.linear1.bias', 'encoder.layers.3.linear2.weight', 'encoder.layers.3.linear2.bias', 'encoder.layers.3.norm1.weight', 'encoder.layers.3.norm1.bias', 'encoder.layers.3.norm2.weight', 'encoder.layers.3.norm2.bias', 'encoder.layers.4.self_attn.in_proj_weight', 'encoder.layers.4.self_attn.in_proj_bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.linear1.weight', 'encoder.layers.4.linear1.bias', 'encoder.layers.4.linear2.weight', 'encoder.layers.4.linear2.bias', 'encoder.layers.4.norm1.weight', 'encoder.layers.4.norm1.bias', 'encoder.layers.4.norm2.weight', 'encoder.layers.4.norm2.bias', 'encoder.layers.5.self_attn.in_proj_weight', 'encoder.layers.5.self_attn.in_proj_bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.linear1.weight', 'encoder.layers.5.linear1.bias', 'encoder.layers.5.linear2.weight', 'encoder.layers.5.linear2.bias', 'encoder.layers.5.norm1.weight', 'encoder.layers.5.norm1.bias', 'encoder.layers.5.norm2.weight', 'encoder.layers.5.norm2.bias', 'encoder.norm.weight', 'encoder.norm.bias']\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "model = Moonshot.load_from_checkpoint(\n",
    "  model_path, strict=False, module_only=True, dim_model=512, dim_coords=\"224,224,64\"\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | enc                 | CoordinateEncoder  | 0     \n",
      "1 | loss                | BCEWithLogitsLoss  | 0     \n",
      "2 | fc                  | Linear             | 3.2 M \n",
      "3 | transformer_encoder | TransformerEncoder | 16.8 M\n",
      "4 | emb                 | Embedding          | 267 K \n",
      "5 | decoder             | TransformerDecoder | 25.2 M\n",
      "6 | token_fc            | Linear             | 268 K \n",
      "7 | loss_fn             | CrossEntropyLoss   | 0     \n",
      "8 | log_softmax         | LogSoftmax         | 0     \n",
      "-----------------------------------------------------------\n",
      "45.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "45.7 M    Total params\n",
      "182.944   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]decoder_inputs.size()=torch.Size([5, 39])\n",
      "decoder_mask.size()=torch.Size([5, 39])\n",
      "tgt_mask.size()=torch.Size([39, 39])\n",
      "decoder_embs.size()=torch.Size([39, 5, 512])\n",
      "memory.size()=torch.Size([13, 5, 512])\n",
      "encoder_mask.size()=torch.Size([5, 13])\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             decoder_inputs.size()=torch.Size([5, 41])\n",
      "decoder_mask.size()=torch.Size([5, 41])\n",
      "tgt_mask.size()=torch.Size([41, 41])\n",
      "decoder_embs.size()=torch.Size([41, 5, 512])\n",
      "memory.size()=torch.Size([22, 5, 512])\n",
      "encoder_mask.size()=torch.Size([5, 22])\n",
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s, v_num=39]decoder_inputs.size()=torch.Size([5, 39])\n",
      "decoder_mask.size()=torch.Size([5, 39])\n",
      "tgt_mask.size()=torch.Size([39, 39])\n",
      "decoder_embs.size()=torch.Size([39, 5, 512])\n",
      "memory.size()=torch.Size([13, 5, 512])\n",
      "encoder_mask.size()=torch.Size([5, 13])\n",
      "Epoch 0: 100%|██████████| 1/1 [00:01<00:00,  1.64s/it, v_num=39]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:03<00:00,  3.41s/it, v_num=39]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    limit_test_batches=1,\n",
    "    max_epochs=1, \n",
    ")\n",
    "results = trainer.fit(model, datamodule=gim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0847, -0.0391,  0.0424, -0.0030, -0.0634], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(obj[\"state_dict\"][\"decoder.layers.1.norm1.bias\"][:5])\n",
    "print(model.decoder.layers[1].norm1.bias[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuing the normal code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/migration/migration.py:201: PossibleUserWarning: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "  rank_zero_warn(\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.2.3 to v2.0.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file tempdata/chemformer/model.ckpt`\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_NUM_BEAMS = 10\n",
    "model_path = \"tempdata/chemformer/model.ckpt\"\n",
    "model = BARTModel.load_from_checkpoint(\n",
    "  model_path,\n",
    "  decode_sampler = sampler,\n",
    "  my_tokeniser = tokeniser,\n",
    ")\n",
    "model.cuda().eval()\n",
    "model.num_beams = DEFAULT_NUM_BEAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\"tb_logs\", name=f\"eval_bart_uspto_50\")\n",
    "trainer = Trainer(\n",
    "    logger=logger,\n",
    "    limit_test_batches=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "=== Describing forward input ===\n",
      "==> Key: encoder_input\n",
      "==> Value: <class 'torch.Tensor'>\n",
      "==> Size: torch.Size([38, 2]) torch.cuda.LongTensor torch.int64\n",
      "==> Decoded:  ['C1=COCCC1.COC(=O)CCC(=O)c1ccc(O)cc1O', 'COC(=O)c1cccc(C(=O)O)c1.Nc1cccnc1N']\n",
      "==> value\n",
      "\ttensor([[  2,   2],\n",
      "\t        [272, 272],\n",
      "\t        [274, 285],\n",
      "\t        [280, 272],\n",
      "\t        [272, 275]], device='cuda:0')\n",
      "==> Key: encoder_pad_mask\n",
      "==> Value: <class 'torch.Tensor'>\n",
      "==> Size: torch.Size([38, 2]) torch.cuda.BoolTensor torch.bool\n",
      "==> value\n",
      "\ttensor([[False, False],\n",
      "\t        [False, False],\n",
      "\t        [False, False],\n",
      "\t        [False, False],\n",
      "\t        [False, False]], device='cuda:0')\n",
      "==> Key: decoder_input\n",
      "==> Value: <class 'torch.Tensor'>\n",
      "==> Size: torch.Size([35, 2]) torch.cuda.LongTensor torch.int64\n",
      "==> Decoded:  ['COC(=O)CCC(=O)c1ccc(OC2CCCCO2)cc1O', 'COC(=O)c1cccc(-c2nc3cccnc3[nH]2)c1']\n",
      "==> value\n",
      "\ttensor([[  2,   2],\n",
      "\t        [272, 272],\n",
      "\t        [285, 285],\n",
      "\t        [272, 272],\n",
      "\t        [275, 275]], device='cuda:0')\n",
      "==> Key: decoder_pad_mask\n",
      "==> Value: <class 'torch.Tensor'>\n",
      "==> Size: torch.Size([35, 2]) torch.cuda.BoolTensor torch.bool\n",
      "==> value\n",
      "\ttensor([[False, False],\n",
      "\t        [False, False],\n",
      "\t        [False, False],\n",
      "\t        [False, False],\n",
      "\t        [False, False]], device='cuda:0')\n",
      "==> Key: target\n",
      "==> Value: <class 'torch.Tensor'>\n",
      "==> Size: torch.Size([35, 2]) torch.cuda.LongTensor torch.int64\n",
      "==> Decoded:  ['COC(=O)CCC(=O)c1ccc(OC2CCCCO2)cc1O', 'COC(=O)c1cccc(-c2nc3cccnc3[nH]2)c1']\n",
      "==> value\n",
      "\ttensor([[272, 272],\n",
      "\t        [285, 285],\n",
      "\t        [272, 272],\n",
      "\t        [275, 275],\n",
      "\t        [280, 280]], device='cuda:0')\n",
      "==> Key: target_mask\n",
      "==> Value: <class 'torch.Tensor'>\n",
      "==> Size: torch.Size([35, 2]) torch.cuda.BoolTensor torch.bool\n",
      "==> value\n",
      "\ttensor([[False, False],\n",
      "\t        [False, False],\n",
      "\t        [False, False],\n",
      "\t        [False, False],\n",
      "\t        [False, False]], device='cuda:0')\n",
      "==> Key: target_smiles\n",
      "==> Value: <class 'list'>\n",
      "=== Memory ===\n",
      "torch.Size([38, 2, 512])\n",
      "=== Decoder Embs ===\n",
      "torch.Size([35, 2, 512])\n",
      "=== tgt_mask ===\n",
      "torch.Size([35, 35])\n",
      "\ttensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "\t         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "\t        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "\t         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "\t        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "\t         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "\t        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "\t         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "\t        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "\t         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]],\n",
      "\t       device='cuda:0')\n",
      "Test Step\n",
      "Target Smiles: \n",
      "['COC(=O)CCC(=O)c1ccc(OC2CCCCO2)cc1O', 'COC(=O)c1cccc(-c2nc3cccnc3[nH]2)c1']\n",
      "Mol_strs: \n",
      "[['C1=C(COC(=O)CCC(c2ccc(O)cc2O)=O)CCO1', 'c1(O)ccc(C(=O)CCC(=O)OCC2=COCC2)c(O)c1', 'C1=C(COC(=O)CCC(=O)c2ccc(O)cc2O)CCO1', 'C1=C(COC(=O)CCC(c2c(O)cc(O)cc2)=O)CCO1', 'C1(COC(=O)CCC(c2ccc(O)cc2O)=O)=COCC1', 'C1=C(COC(CCC(=O)c2ccc(O)cc2O)=O)CCO1', 'c1cc(O)cc(O)c1C(=O)CCC(=O)OCC1=COCC1', 'c1cc(O)cc(O)c1C(=O)CCC(OCC1=COCC1)=O', 'C1=C(COC(=O)CCC(=O)c2c(O)cc(O)cc2)CCO1', 'C1(COC(=O)CCC(=O)c2ccc(O)cc2O)=COCC1'], ['c1(Nc2cccnc2N)c(C(OC)=O)cccc1C(O)=O', 'c1(Nc2cccnc2N)c(C(OC)=O)cccc1C(=O)O', 'c1ccnc(N)c1Nc1c(C(OC)=O)cccc1C(O)=O', 'c1ccnc(N)c1Nc1c(C(=O)OC)cccc1C(O)=O', 'c1ccnc(N)c1Nc1c(C(OC)=O)cccc1C(=O)O', 'c1(Nc2cccnc2N)c(C(=O)OC)cccc1C(O)=O', 'c1ccnc(N)c1Nc1c(C(=O)OC)cccc1C(=O)O', 'c1(Nc2cccnc2N)c(C(=O)OC)cccc1C(=O)O', 'c1(C(O)=O)cccc(C(OC)=O)c1Nc1cccnc1N', 'c1(C(OC)=O)cccc(C(O)=O)c1Nc1cccnc1N']]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:04<00:00,  4.41s/it]\n",
      "Results for model: tempdata/chemformer/model.ckpt\n",
      "Item                     Result\n"
     ]
    }
   ],
   "source": [
    "results = trainer.test(model, datamodule=dm)\n",
    "def print_results(results):\n",
    "  print(f\"Results for model: {model_path}\")\n",
    "  print(f\"{'Item':<25}Result\")\n",
    "  for key, val in results.items():\n",
    "    print(f\"{key:<25} {val:.4f}\")\n",
    "print_results(results[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
