{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04/14/23\n",
    "\n",
    "This notebook is intended to test chemformer loading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from models.chemformer.molbart import BARTModel\n",
    "from models.chemformer.tokeniser import MolEncTokeniser\n",
    "from models.chemformer.utils import REGEX, DEFAULT_MAX_SEQ_LEN\n",
    "from models.chemformer.molbart_dataset import Uspto50\n",
    "from models.chemformer.molbart_datamodule import FineTuneReactionDataModule\n",
    "from models.chemformer.decoder import DecodeSampler\n",
    "\n",
    "from models.ranked_transformer import Moonshot\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model. Using code from the original chemformer repo (```molbart/util.py```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary\n",
    "from pathlib import Path\n",
    "vocab_path = \"tempdata/chemformer/bart_vocab.txt\"\n",
    "chem_token_start = 272\n",
    "tokeniser = MolEncTokeniser.from_vocab_file(\n",
    "  vocab_path, REGEX, chem_token_start\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uspto50 __init()__: \n",
      "[DS] <class 'pandas.core.frame.DataFrame'> 50037\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "data_path = \"tempdata/chemformer/uspto_50.pickle\"\n",
    "aug_prob = 0.0\n",
    "dataset = Uspto50(\n",
    "  data_path, aug_prob, forward=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a batch size of 2.\n",
      "Building data module for forward prediction task...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model\n",
    "model_type = \"bart\"\n",
    "uni_model = model_type == \"unified\"\n",
    "batch_size = 2\n",
    "train_tokens = None\n",
    "num_buckets = None\n",
    "\n",
    "dm = FineTuneReactionDataModule(\n",
    "    dataset,\n",
    "    tokeniser,\n",
    "    batch_size,\n",
    "    DEFAULT_MAX_SEQ_LEN,\n",
    "    forward_pred=True,\n",
    "    val_idxs=dataset.val_idxs,\n",
    "    test_idxs=dataset.test_idxs,\n",
    "    train_token_batch_size=train_tokens,\n",
    "    num_buckets=num_buckets,\n",
    "    unified_model=uni_model\n",
    ")\n",
    "sampler = DecodeSampler(tokeniser, DEFAULT_MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_path = \"tempdata/chemformer/model.ckpt\"\n",
    "obj = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0847, -0.0391,  0.0424, -0.0030, -0.0634], device='cuda:0',\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj[\"state_dict\"][\"decoder.layers.1.norm1.bias\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.2.3 to v2.0.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file tempdata/chemformer/model.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed an encoder with no defined bounds\n",
      "Pushed an encoder with no defined bounds\n",
      "Pushed an encoder with no defined bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:153: UserWarning: Found keys that are in the model state dict but not in the checkpoint: ['latent', 'fc.weight', 'fc.bias', 'transformer_encoder.layers.0.self_attn.in_proj_weight', 'transformer_encoder.layers.0.self_attn.in_proj_bias', 'transformer_encoder.layers.0.self_attn.out_proj.weight', 'transformer_encoder.layers.0.self_attn.out_proj.bias', 'transformer_encoder.layers.0.linear1.weight', 'transformer_encoder.layers.0.linear1.bias', 'transformer_encoder.layers.0.linear2.weight', 'transformer_encoder.layers.0.linear2.bias', 'transformer_encoder.layers.0.norm1.weight', 'transformer_encoder.layers.0.norm1.bias', 'transformer_encoder.layers.0.norm2.weight', 'transformer_encoder.layers.0.norm2.bias', 'transformer_encoder.layers.1.self_attn.in_proj_weight', 'transformer_encoder.layers.1.self_attn.in_proj_bias', 'transformer_encoder.layers.1.self_attn.out_proj.weight', 'transformer_encoder.layers.1.self_attn.out_proj.bias', 'transformer_encoder.layers.1.linear1.weight', 'transformer_encoder.layers.1.linear1.bias', 'transformer_encoder.layers.1.linear2.weight', 'transformer_encoder.layers.1.linear2.bias', 'transformer_encoder.layers.1.norm1.weight', 'transformer_encoder.layers.1.norm1.bias', 'transformer_encoder.layers.1.norm2.weight', 'transformer_encoder.layers.1.norm2.bias', 'transformer_encoder.layers.2.self_attn.in_proj_weight', 'transformer_encoder.layers.2.self_attn.in_proj_bias', 'transformer_encoder.layers.2.self_attn.out_proj.weight', 'transformer_encoder.layers.2.self_attn.out_proj.bias', 'transformer_encoder.layers.2.linear1.weight', 'transformer_encoder.layers.2.linear1.bias', 'transformer_encoder.layers.2.linear2.weight', 'transformer_encoder.layers.2.linear2.bias', 'transformer_encoder.layers.2.norm1.weight', 'transformer_encoder.layers.2.norm1.bias', 'transformer_encoder.layers.2.norm2.weight', 'transformer_encoder.layers.2.norm2.bias', 'transformer_encoder.layers.3.self_attn.in_proj_weight', 'transformer_encoder.layers.3.self_attn.in_proj_bias', 'transformer_encoder.layers.3.self_attn.out_proj.weight', 'transformer_encoder.layers.3.self_attn.out_proj.bias', 'transformer_encoder.layers.3.linear1.weight', 'transformer_encoder.layers.3.linear1.bias', 'transformer_encoder.layers.3.linear2.weight', 'transformer_encoder.layers.3.linear2.bias', 'transformer_encoder.layers.3.norm1.weight', 'transformer_encoder.layers.3.norm1.bias', 'transformer_encoder.layers.3.norm2.weight', 'transformer_encoder.layers.3.norm2.bias', 'transformer_encoder.layers.4.self_attn.in_proj_weight', 'transformer_encoder.layers.4.self_attn.in_proj_bias', 'transformer_encoder.layers.4.self_attn.out_proj.weight', 'transformer_encoder.layers.4.self_attn.out_proj.bias', 'transformer_encoder.layers.4.linear1.weight', 'transformer_encoder.layers.4.linear1.bias', 'transformer_encoder.layers.4.linear2.weight', 'transformer_encoder.layers.4.linear2.bias', 'transformer_encoder.layers.4.norm1.weight', 'transformer_encoder.layers.4.norm1.bias', 'transformer_encoder.layers.4.norm2.weight', 'transformer_encoder.layers.4.norm2.bias', 'transformer_encoder.layers.5.self_attn.in_proj_weight', 'transformer_encoder.layers.5.self_attn.in_proj_bias', 'transformer_encoder.layers.5.self_attn.out_proj.weight', 'transformer_encoder.layers.5.self_attn.out_proj.bias', 'transformer_encoder.layers.5.linear1.weight', 'transformer_encoder.layers.5.linear1.bias', 'transformer_encoder.layers.5.linear2.weight', 'transformer_encoder.layers.5.linear2.bias', 'transformer_encoder.layers.5.norm1.weight', 'transformer_encoder.layers.5.norm1.bias', 'transformer_encoder.layers.5.norm2.weight', 'transformer_encoder.layers.5.norm2.bias', 'transformer_encoder.layers.6.self_attn.in_proj_weight', 'transformer_encoder.layers.6.self_attn.in_proj_bias', 'transformer_encoder.layers.6.self_attn.out_proj.weight', 'transformer_encoder.layers.6.self_attn.out_proj.bias', 'transformer_encoder.layers.6.linear1.weight', 'transformer_encoder.layers.6.linear1.bias', 'transformer_encoder.layers.6.linear2.weight', 'transformer_encoder.layers.6.linear2.bias', 'transformer_encoder.layers.6.norm1.weight', 'transformer_encoder.layers.6.norm1.bias', 'transformer_encoder.layers.6.norm2.weight', 'transformer_encoder.layers.6.norm2.bias', 'transformer_encoder.layers.7.self_attn.in_proj_weight', 'transformer_encoder.layers.7.self_attn.in_proj_bias', 'transformer_encoder.layers.7.self_attn.out_proj.weight', 'transformer_encoder.layers.7.self_attn.out_proj.bias', 'transformer_encoder.layers.7.linear1.weight', 'transformer_encoder.layers.7.linear1.bias', 'transformer_encoder.layers.7.linear2.weight', 'transformer_encoder.layers.7.linear2.bias', 'transformer_encoder.layers.7.norm1.weight', 'transformer_encoder.layers.7.norm1.bias', 'transformer_encoder.layers.7.norm2.weight', 'transformer_encoder.layers.7.norm2.bias']\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:157: UserWarning: Found keys that are not in the model state dict but in the checkpoint: ['pos_emb', 'emb.weight', 'encoder.layers.0.self_attn.in_proj_weight', 'encoder.layers.0.self_attn.in_proj_bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.linear1.weight', 'encoder.layers.0.linear1.bias', 'encoder.layers.0.linear2.weight', 'encoder.layers.0.linear2.bias', 'encoder.layers.0.norm1.weight', 'encoder.layers.0.norm1.bias', 'encoder.layers.0.norm2.weight', 'encoder.layers.0.norm2.bias', 'encoder.layers.1.self_attn.in_proj_weight', 'encoder.layers.1.self_attn.in_proj_bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.linear1.weight', 'encoder.layers.1.linear1.bias', 'encoder.layers.1.linear2.weight', 'encoder.layers.1.linear2.bias', 'encoder.layers.1.norm1.weight', 'encoder.layers.1.norm1.bias', 'encoder.layers.1.norm2.weight', 'encoder.layers.1.norm2.bias', 'encoder.layers.2.self_attn.in_proj_weight', 'encoder.layers.2.self_attn.in_proj_bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.linear1.weight', 'encoder.layers.2.linear1.bias', 'encoder.layers.2.linear2.weight', 'encoder.layers.2.linear2.bias', 'encoder.layers.2.norm1.weight', 'encoder.layers.2.norm1.bias', 'encoder.layers.2.norm2.weight', 'encoder.layers.2.norm2.bias', 'encoder.layers.3.self_attn.in_proj_weight', 'encoder.layers.3.self_attn.in_proj_bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.linear1.weight', 'encoder.layers.3.linear1.bias', 'encoder.layers.3.linear2.weight', 'encoder.layers.3.linear2.bias', 'encoder.layers.3.norm1.weight', 'encoder.layers.3.norm1.bias', 'encoder.layers.3.norm2.weight', 'encoder.layers.3.norm2.bias', 'encoder.layers.4.self_attn.in_proj_weight', 'encoder.layers.4.self_attn.in_proj_bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.linear1.weight', 'encoder.layers.4.linear1.bias', 'encoder.layers.4.linear2.weight', 'encoder.layers.4.linear2.bias', 'encoder.layers.4.norm1.weight', 'encoder.layers.4.norm1.bias', 'encoder.layers.4.norm2.weight', 'encoder.layers.4.norm2.bias', 'encoder.layers.5.self_attn.in_proj_weight', 'encoder.layers.5.self_attn.in_proj_bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.linear1.weight', 'encoder.layers.5.linear1.bias', 'encoder.layers.5.linear2.weight', 'encoder.layers.5.linear2.bias', 'encoder.layers.5.norm1.weight', 'encoder.layers.5.norm1.bias', 'encoder.layers.5.norm2.weight', 'encoder.layers.5.norm2.bias', 'encoder.norm.weight', 'encoder.norm.bias']\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "model = Moonshot.load_from_checkpoint(\n",
    "  model_path, strict=False, module_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 8.4656e-02, -3.9124e-02,  4.2358e-02, -2.9659e-03, -6.3416e-02,\n",
       "         5.7144e-03,  5.1422e-02, -6.0730e-03,  5.1422e-02,  2.6978e-02,\n",
       "        -1.2268e-02, -2.4185e-02,  2.2629e-02, -5.2414e-03, -7.7858e-03,\n",
       "        -1.6327e-02,  1.2988e-01,  4.4067e-02,  7.1831e-03, -7.4341e-02,\n",
       "        -5.4047e-02, -4.5685e-02,  7.1289e-02,  5.9906e-02, -4.3213e-02,\n",
       "         8.9539e-02, -3.7964e-02,  4.5380e-02,  8.7830e-02, -1.9241e-02,\n",
       "        -3.9795e-02,  6.2469e-02,  3.8727e-02,  3.9551e-02,  8.6975e-02,\n",
       "         3.9406e-03, -2.1469e-02, -2.2461e-02,  4.3106e-03, -8.3191e-02,\n",
       "         5.9418e-02,  1.5796e-01, -6.6872e-03, -4.7516e-02, -2.3479e-03,\n",
       "         1.8326e-02,  1.3904e-01, -6.1218e-02,  4.3640e-03,  2.6184e-02,\n",
       "        -4.4800e-02,  1.0162e-01, -3.1433e-02,  8.6288e-03,  2.9648e-02,\n",
       "         5.3741e-02, -3.7018e-02, -1.5701e-02,  2.9587e-02,  1.4099e-01,\n",
       "         5.2376e-03,  3.1860e-02,  1.1719e-01, -2.5055e-02, -2.7676e-03,\n",
       "         6.0150e-02, -8.0688e-02,  6.0760e-02, -1.9669e-02,  7.4158e-02,\n",
       "         5.2002e-01,  6.9618e-03, -8.9478e-02,  1.0822e-01, -5.2887e-02,\n",
       "         1.3428e-01,  7.4890e-02,  1.4496e-03,  1.7017e-01,  6.0455e-02,\n",
       "         1.1208e-02,  1.2558e-02,  5.2147e-03,  3.4393e-02, -8.1406e-03,\n",
       "         3.0914e-02,  2.4471e-03,  2.2995e-02, -5.2223e-03, -3.8025e-02,\n",
       "         3.0960e-02,  5.7190e-02,  1.7838e-02,  6.0120e-03, -6.1188e-03,\n",
       "         2.1347e-02,  7.7705e-03,  6.0608e-02,  5.9052e-02, -2.9556e-02,\n",
       "         6.8359e-02,  2.5070e-02, -2.7725e-02,  2.6154e-02,  5.3467e-02,\n",
       "        -7.0763e-03,  5.5618e-03, -2.2491e-02,  5.3680e-02, -1.5907e-03,\n",
       "         3.1708e-02, -4.1412e-02, -6.7444e-02,  4.6295e-02,  3.2623e-02,\n",
       "        -1.1523e-01,  2.4643e-02,  7.5745e-02, -3.5034e-02,  8.2642e-02,\n",
       "         1.6006e-02,  2.2522e-02, -2.2995e-02, -2.1973e-02, -3.9093e-02,\n",
       "        -5.2567e-03, -3.5309e-02, -3.0945e-02,  1.9241e-02,  6.6956e-02,\n",
       "        -3.9612e-02,  3.3234e-02, -3.1158e-02,  8.7830e-02, -2.2781e-02,\n",
       "         1.6760e-01, -2.7199e-03,  2.0063e-04,  5.3902e-03, -4.0588e-03,\n",
       "        -2.3056e-02,  5.1460e-03, -2.2491e-02, -7.5134e-02, -7.8796e-02,\n",
       "         1.2112e-03, -3.1677e-02, -4.1687e-02, -2.3132e-02, -9.5291e-03,\n",
       "         5.2605e-03, -4.8584e-02, -4.0131e-03, -7.2144e-02, -4.1809e-03,\n",
       "        -4.1412e-02, -2.7447e-03, -1.3077e-02, -1.7456e-02, -9.9564e-03,\n",
       "        -1.4412e-02, -3.4027e-02,  1.1658e-02, -4.9683e-02, -3.7323e-02,\n",
       "        -1.0742e-01,  1.6632e-02, -1.7319e-02, -1.1757e-02, -5.5580e-03,\n",
       "         3.0045e-02, -5.1453e-02, -1.1658e-02, -2.0340e-02, -4.5624e-02,\n",
       "        -1.8234e-02,  9.1370e-02, -3.8727e-02,  7.7095e-03, -5.6976e-02,\n",
       "         2.3392e-02, -4.3488e-02, -1.4183e-02, -3.4302e-02, -6.3416e-02,\n",
       "        -1.4359e-02,  8.4763e-03, -2.3376e-02, -2.0218e-02,  1.1780e-02,\n",
       "        -9.3365e-04,  4.1718e-02, -4.1473e-02,  1.3672e-02,  2.6550e-02,\n",
       "        -2.3413e-01,  1.5228e-02, -6.6772e-02,  1.5083e-02, -2.2537e-02,\n",
       "        -1.1047e-01, -2.7328e-02,  2.3300e-02, -5.3253e-02, -3.4485e-02,\n",
       "        -4.0924e-02, -2.2476e-02,  1.4786e-02,  1.4519e-02, -4.8126e-02,\n",
       "        -2.9831e-03, -5.4718e-02,  2.3270e-02, -3.5400e-02, -1.6584e-03,\n",
       "        -3.4424e-02,  4.5166e-02, -3.4637e-02, -3.4237e-03, -9.4681e-03,\n",
       "        -8.0261e-03, -2.9373e-02,  9.0714e-03, -4.7266e-01,  3.9520e-02,\n",
       "        -3.5645e-02,  3.4485e-02, -4.9744e-02,  2.8427e-02, -7.0007e-02,\n",
       "        -6.0997e-03, -2.1027e-02,  6.4182e-04, -2.1896e-02, -1.0536e-02,\n",
       "        -4.6539e-02,  2.9678e-02,  5.3619e-02,  8.8684e-02, -1.5717e-02,\n",
       "        -1.5823e-02, -5.9296e-02,  3.4218e-03, -1.6357e-01,  8.8425e-03,\n",
       "        -6.2103e-02,  2.5574e-02, -3.1021e-02, -1.6495e-02, -1.2947e-02,\n",
       "        -8.6365e-03,  1.2299e-02,  6.0577e-03, -2.1286e-02, -7.4883e-03,\n",
       "        -4.4586e-02,  1.1642e-02, -1.8707e-02,  8.7708e-02, -4.0558e-02,\n",
       "         3.3630e-02, -6.0547e-02,  2.0599e-02, -6.4270e-02, -2.2415e-02,\n",
       "        -1.5854e-02, -3.0346e-03, -1.0809e-01,  4.6463e-03,  1.3176e-02,\n",
       "         7.3471e-03, -4.2877e-02,  1.4091e-02, -8.5388e-02,  2.7603e-02,\n",
       "         4.2000e-03,  4.7394e-02, -2.2980e-02,  1.6754e-02,  2.5665e-02,\n",
       "         3.2163e-04, -1.3977e-01,  5.4230e-02, -1.9440e-02,  1.5984e-03,\n",
       "        -1.1742e-02, -5.6366e-02,  2.2488e-03,  2.1088e-02, -1.0223e-02,\n",
       "        -2.2079e-02, -5.6648e-03,  3.4058e-02,  1.2047e-02,  2.7328e-02,\n",
       "        -1.0535e-01, -6.8398e-03, -2.4927e-01,  3.0853e-02, -6.7932e-02,\n",
       "        -1.7593e-02, -4.0802e-02, -1.5717e-02, -5.2063e-02,  1.6556e-02,\n",
       "        -1.0490e-02,  4.2969e-02, -1.8921e-02,  3.0792e-02,  1.2321e-02,\n",
       "         2.1301e-02, -5.9418e-02, -1.4374e-02,  2.1713e-02, -2.0172e-02,\n",
       "        -6.5796e-02,  2.6825e-02,  1.1797e-03,  1.6846e-02, -9.6893e-03,\n",
       "         9.7351e-02, -1.3329e-02, -3.2837e-02, -2.2461e-02,  8.5831e-03,\n",
       "        -1.4807e-01,  5.9853e-03, -2.1179e-02,  6.0455e-02, -4.7943e-02,\n",
       "         8.3542e-03, -2.9861e-02,  4.8309e-02, -2.8870e-02,  3.3905e-02,\n",
       "        -1.7029e-02,  5.5618e-03, -6.4453e-02,  1.4862e-02,  1.2012e-01,\n",
       "         1.6663e-02, -1.3527e-02,  5.3520e-03, -2.8961e-02,  6.8848e-01,\n",
       "         4.7302e-04, -2.0676e-02,  6.8245e-03,  1.4626e-02, -3.9307e-02,\n",
       "         2.7023e-02, -2.8534e-02, -2.8290e-02, -5.3284e-02,  1.3191e-02,\n",
       "        -3.5919e-02,  4.2145e-02, -7.0435e-02,  3.6373e-03, -2.1637e-02,\n",
       "         5.5939e-02, -6.0364e-02,  1.9760e-02, -3.6499e-02,  3.1021e-02,\n",
       "        -1.0498e-01,  3.9001e-02, -1.6602e-02,  4.5685e-02,  3.3245e-03,\n",
       "        -4.4708e-03, -5.6458e-02,  7.5531e-03, -8.1909e-02, -8.8882e-03,\n",
       "        -5.9875e-02,  7.9956e-02, -4.2542e-02,  3.9948e-02, -2.3071e-02,\n",
       "         3.9307e-02, -1.6766e-03,  5.4657e-02, -2.8152e-02,  1.0425e-01,\n",
       "        -1.0297e-01, -2.4628e-02,  1.3077e-02,  2.9907e-03, -2.3071e-02,\n",
       "         1.8921e-01, -3.0746e-02,  5.3528e-02, -4.9561e-02,  1.4641e-02,\n",
       "         3.7861e-03,  2.1851e-02, -1.6602e-02, -2.6260e-02, -3.6926e-02,\n",
       "         3.2562e-02, -2.0782e-02,  5.7587e-02, -7.1594e-02, -8.0414e-03,\n",
       "        -1.8372e-02,  3.1097e-02, -1.9318e-02,  4.2114e-02, -1.4206e-02,\n",
       "         5.0354e-02, -3.3508e-02,  3.8300e-02, -7.7868e-04,  5.5542e-03,\n",
       "        -2.5543e-02, -2.6722e-03, -1.8967e-02,  1.0413e-01, -4.3030e-02,\n",
       "         6.1584e-02, -3.1555e-02,  1.4626e-02, -5.4443e-02,  5.6061e-02,\n",
       "        -1.1316e-01,  9.9564e-03, -6.9519e-02, -5.7716e-03, -2.4475e-02,\n",
       "        -3.8727e-02, -6.1310e-02,  4.8859e-02, -1.3626e-02,  8.7051e-03,\n",
       "        -3.1372e-02,  5.8746e-02, -7.8087e-03,  2.5436e-02, -2.7527e-02,\n",
       "         5.5511e-02, -5.0293e-02,  1.1133e-01, -1.3474e-02,  6.6223e-02,\n",
       "        -3.8422e-02,  2.7161e-02, -7.7332e-02,  2.6901e-02, -2.1927e-02,\n",
       "         2.9739e-02, -7.9407e-02, -2.2232e-02, -7.8979e-02,  1.2306e-02,\n",
       "        -1.9394e-02, -1.5182e-03, -3.8971e-02,  4.1138e-02, -3.6804e-02,\n",
       "         2.4719e-02, -2.9007e-02,  2.1768e-04, -2.8564e-02,  1.9684e-02,\n",
       "        -5.9961e-01,  1.5007e-02, -9.5444e-03,  2.6810e-02,  2.6764e-02,\n",
       "        -6.2622e-02, -2.5635e-02,  5.5359e-02,  3.9978e-03,  4.7424e-02,\n",
       "        -6.6528e-02,  5.2002e-02, -1.2733e-02,  1.5038e-02,  8.3771e-03,\n",
       "         6.1584e-02,  7.9880e-03,  5.1941e-02, -1.8173e-02, -2.6001e-02,\n",
       "        -4.7534e-01,  3.3817e-03, -2.5482e-02,  4.1138e-02,  1.4305e-02,\n",
       "        -4.1016e-02, -2.1027e-02,  1.2140e-03, -1.4275e-02, -3.0762e-02,\n",
       "        -8.7891e-03,  5.5786e-02, -5.6244e-02, -4.6501e-03,  1.1444e-02,\n",
       "        -1.9485e-02, -8.8501e-02,  2.3174e-03, -9.5367e-03, -8.3618e-03,\n",
       "        -3.8239e-02,  2.1820e-02,  1.7996e-03, -6.8665e-03, -1.5282e-02,\n",
       "         3.0869e-02, -2.8839e-03], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.layers[1].norm1.bias[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.2.3 to v2.0.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file tempdata/chemformer/model.ckpt`\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_NUM_BEAMS = 10\n",
    "model_path = \"tempdata/chemformer/model.ckpt\"\n",
    "model = BARTModel.load_from_checkpoint(\n",
    "  model_path,\n",
    "  decode_sampler = sampler,\n",
    "  my_tokeniser = tokeniser,\n",
    ")\n",
    "model.cuda().eval()\n",
    "model.num_beams = DEFAULT_NUM_BEAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\"tb_logs\", name=f\"eval_bart_uspto_50\")\n",
    "trainer = Trainer(\n",
    "    logger=logger,\n",
    "    limit_test_batches=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:55<?, ?it/s]\n",
      "\n",
      "=== Describing forward input ===\n",
      "==> Key: encoder_input\n",
      "==> Value: <class 'torch.Tensor'>\n",
      "==> Size: torch.Size([38, 2]) torch.cuda.LongTensor torch.int64\n",
      "==> Decoded:  ['C1=COCCC1.COC(=O)CCC(=O)c1ccc(O)cc1O', 'COC(=O)c1cccc(C(=O)O)c1.Nc1cccnc1N']\n",
      "==> value\n",
      "\ttensor([[  2,   2],\n",
      "\t        [272, 272],\n",
      "\t        [274, 285],\n",
      "\t        [280, 272],\n",
      "\t        [272, 275]], device='cuda:0')\n",
      "==> Key: encoder_pad_mask\n",
      "==> Value: <class 'torch.Tensor'>\n",
      "==> Size: torch.Size([38, 2]) torch.cuda.BoolTensor torch.bool\n",
      "==> value\n",
      "\ttensor([[False, False],\n",
      "\t        [False, False],\n",
      "\t        [False, False],\n",
      "\t        [False, False],\n",
      "\t        [False, False]], device='cuda:0')\n",
      "==> Key: decoder_input\n",
      "==> Value: <class 'torch.Tensor'>\n",
      "==> Size: torch.Size([35, 2]) torch.cuda.LongTensor torch.int64\n",
      "==> Decoded:  ['COC(=O)CCC(=O)c1ccc(OC2CCCCO2)cc1O', 'COC(=O)c1cccc(-c2nc3cccnc3[nH]2)c1']\n",
      "==> value\n",
      "\ttensor([[  2,   2],\n",
      "\t        [272, 272],\n",
      "\t        [285, 285],\n",
      "\t        [272, 272],\n",
      "\t        [275, 275]], device='cuda:0')\n",
      "==> Key: decoder_pad_mask\n",
      "==> Value: <class 'torch.Tensor'>\n",
      "==> Size: torch.Size([35, 2]) torch.cuda.BoolTensor torch.bool\n",
      "==> value\n",
      "\ttensor([[False, False],\n",
      "\t        [False, False],\n",
      "\t        [False, False],\n",
      "\t        [False, False],\n",
      "\t        [False, False]], device='cuda:0')\n",
      "==> Key: target\n",
      "==> Value: <class 'torch.Tensor'>\n",
      "==> Size: torch.Size([35, 2]) torch.cuda.LongTensor torch.int64\n",
      "==> Decoded:  ['COC(=O)CCC(=O)c1ccc(OC2CCCCO2)cc1O', 'COC(=O)c1cccc(-c2nc3cccnc3[nH]2)c1']\n",
      "==> value\n",
      "\ttensor([[272, 272],\n",
      "\t        [285, 285],\n",
      "\t        [272, 272],\n",
      "\t        [275, 275],\n",
      "\t        [280, 280]], device='cuda:0')\n",
      "==> Key: target_mask\n",
      "==> Value: <class 'torch.Tensor'>\n",
      "==> Size: torch.Size([35, 2]) torch.cuda.BoolTensor torch.bool\n",
      "==> value\n",
      "\ttensor([[False, False],\n",
      "\t        [False, False],\n",
      "\t        [False, False],\n",
      "\t        [False, False],\n",
      "\t        [False, False]], device='cuda:0')\n",
      "==> Key: target_smiles\n",
      "==> Value: <class 'list'>\n",
      "=== Memory ===\n",
      "torch.Size([38, 2, 512])\n",
      "Test Step\n",
      "Target Smiles: \n",
      "['COC(=O)CCC(=O)c1ccc(OC2CCCCO2)cc1O', 'COC(=O)c1cccc(-c2nc3cccnc3[nH]2)c1']\n",
      "Mol_strs: \n",
      "[['C1=C(COC(=O)CCC(c2ccc(O)cc2O)=O)CCO1', 'c1(O)ccc(C(=O)CCC(=O)OCC2=COCC2)c(O)c1', 'C1=C(COC(=O)CCC(=O)c2ccc(O)cc2O)CCO1', 'C1=C(COC(=O)CCC(c2c(O)cc(O)cc2)=O)CCO1', 'C1(COC(=O)CCC(c2ccc(O)cc2O)=O)=COCC1', 'C1=C(COC(CCC(=O)c2ccc(O)cc2O)=O)CCO1', 'c1cc(O)cc(O)c1C(=O)CCC(=O)OCC1=COCC1', 'c1cc(O)cc(O)c1C(=O)CCC(OCC1=COCC1)=O', 'C1=C(COC(=O)CCC(=O)c2c(O)cc(O)cc2)CCO1', 'C1(COC(=O)CCC(=O)c2ccc(O)cc2O)=COCC1'], ['c1(Nc2cccnc2N)c(C(OC)=O)cccc1C(O)=O', 'c1(Nc2cccnc2N)c(C(OC)=O)cccc1C(=O)O', 'c1ccnc(N)c1Nc1c(C(OC)=O)cccc1C(O)=O', 'c1ccnc(N)c1Nc1c(C(=O)OC)cccc1C(O)=O', 'c1ccnc(N)c1Nc1c(C(OC)=O)cccc1C(=O)O', 'c1(Nc2cccnc2N)c(C(=O)OC)cccc1C(O)=O', 'c1ccnc(N)c1Nc1c(C(=O)OC)cccc1C(=O)O', 'c1(Nc2cccnc2N)c(C(=O)OC)cccc1C(=O)O', 'c1(C(O)=O)cccc(C(OC)=O)c1Nc1cccnc1N', 'c1(C(OC)=O)cccc(C(O)=O)c1Nc1cccnc1N']]\n",
      "Testing: 1it [00:04,  4.63s/it]\n",
      "Results for model: tempdata/chemformer/model.ckpt\n",
      "Item                     Result\n"
     ]
    }
   ],
   "source": [
    "results = trainer.test(model, datamodule=dm)\n",
    "def print_results(results):\n",
    "  print(f\"Results for model: {model_path}\")\n",
    "  print(f\"{'Item':<25}Result\")\n",
    "  for key, val in results.items():\n",
    "    print(f\"{key:<25} {val:.4f}\")\n",
    "print_results(results[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
